{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d663975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pyflagser\n",
    "import statistics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d432ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path structure\n",
    "file_path_template = \"Features/{datasetname}{attribute}_degcent_subm2.csv\"\n",
    "\n",
    "# Function to load dataset by name\n",
    "def load_dataset(datasetname):\n",
    "    # Construct the file path\n",
    "    b0_path = file_path_template.format(datasetname=datasetname,attribute='B0')\n",
    "    b1_path = file_path_template.format(datasetname=datasetname,attribute='B1')\n",
    "    c0_path = file_path_template.format(datasetname=datasetname,attribute='c0')\n",
    "    c1_path = file_path_template.format(datasetname=datasetname,attribute='c1')\n",
    "    \n",
    "    try:\n",
    "        # Load the CSV file into a DataFrame\n",
    "        b0_data = pd.read_csv(b0_path)\n",
    "        b1_data = pd.read_csv(b1_path)\n",
    "        c0_data = pd.read_csv(c0_path)\n",
    "        c1_data = pd.read_csv(c1_path)\n",
    "        b1_data = b1_data.drop(columns=['Unnamed: 0'])\n",
    "        b0_data = b0_data.drop(columns=['Unnamed: 0'])\n",
    "        c1_data = c1_data.drop(columns=['Unnamed: 0'])\n",
    "        c0_data = c0_data.drop(columns=['Unnamed: 0'])\n",
    "        b0_array = b0_data.to_numpy()\n",
    "        b1_array = b1_data.to_numpy()\n",
    "        c0_array = c0_data.to_numpy()\n",
    "        c1_array = c1_data.to_numpy()\n",
    "\n",
    "        # Combine the arrays into a 3D array\n",
    "        combined_array = np.stack((b0_array, b1_array,c0_array,c1_array), axis=-1)\n",
    "        print(f\"Successfully loaded dataset: {datasetname} features\")\n",
    "        return combined_array\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File for dataset '{datasetname}' not found.\")\n",
    "        return None\n",
    "def load_label(dataset):\n",
    "    if dataset=='PROTEINS':\n",
    "        url='https://raw.githubusercontent.com/AstritTola/TopER/refs/heads/main/Datasets/PROTEINS/PROTEINS_graph_labels.txt'\n",
    "        graph_label=np.loadtxt(url)\n",
    "        max_value = np.max(graph_label)\n",
    "        graph_label[graph_label == max_value] = 0 #start graph label with 0\n",
    "    elif dataset=='BZR':\n",
    "        url='https://raw.githubusercontent.com/AstritTola/TopER/refs/heads/main/Datasets/BZR/BZR_graph_labels.txt'\n",
    "        graph_label=np.loadtxt(url)\n",
    "        min_value = np.min(graph_label)\n",
    "        graph_label[graph_label == min_value] = 0 #start graph label with 0\n",
    "    elif dataset=='COX2':\n",
    "        url='https://raw.githubusercontent.com/AstritTola/TopER/refs/heads/main/Datasets/COX2/COX2_graph_labels.txt'\n",
    "        graph_label=np.loadtxt(url)\n",
    "        min_value = np.min(graph_label)\n",
    "        graph_label[graph_label == min_value] = 0 #start graph label with 0\n",
    "    elif dataset=='MUTAG':\n",
    "        url='https://raw.githubusercontent.com/AstritTola/TopER/refs/heads/main/Datasets/MUTAG/MUTAG_graph_labels.txt'\n",
    "        graph_label=np.loadtxt(url)\n",
    "        min_value = np.min(graph_label)\n",
    "        graph_label[graph_label == min_value] = 0 #start graph label with 0\n",
    "    elif dataset=='IMDB-BINARY':\n",
    "        url='https://raw.githubusercontent.com/AstritTola/TopER/refs/heads/main/Datasets/IMDB-BINARY/IMDB-BINARY_graph_labels.txt'\n",
    "        graph_label=np.loadtxt(url)\n",
    "    elif dataset=='IMDB-MULTI':\n",
    "        url='https://raw.githubusercontent.com/AstritTola/TopER/refs/heads/main/Datasets/IMDB-MULTI/IMDB-MULTI_graph_labels.txt'\n",
    "        graph_label=np.loadtxt(url)\n",
    "        max_value = np.max(graph_label)\n",
    "        graph_label[graph_label == max_value] = 0 #start graph label with 0\n",
    "    elif dataset=='REDDIT-BINARY':\n",
    "        graph_label=np.loadtxt('REDDIT-BINARY/REDDIT-BINARY_graph_labels.txt')\n",
    "        min_value = np.min(graph_label)\n",
    "        graph_label[graph_label == min_value] = 0 #start graph label with 0\n",
    "        \n",
    "    else:\n",
    "        print('Label not avilable')\n",
    "#     graph_label=np.loadtxt(url)\n",
    "#     max_value = np.max(graph_label)\n",
    "#     graph_label[graph_label == max_value] = 0 #start graph label with 0 \n",
    "    return graph_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3dc248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat(acc_list):\n",
    "    mean = statistics.mean(acc_list)\n",
    "    stdev = statistics.stdev(acc_list)\n",
    "    print(f'Final Accuracy using 10 fold CV: {mean*100:.2f} \\u00B1 {stdev*100:.2f}%')\n",
    "    #print(\"Final Accuracy using 10 fold CV:\", mean:.2f, \"\\u00B1\", stdev,\"\\n\")\n",
    "def print_stat(train_acc, test_acc):\n",
    "    argmax=np.argmax(train_acc)\n",
    "    best_result=test_acc[argmax]\n",
    "    print(f'Test Accuracy = {best_result:.2f}%\\n')\n",
    "    return best_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2d5bfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=read_feature('REDDIT-BINARY')\n",
    "graph_label=load_label('REDDIT-BINARY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4ccfbfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=read_feature('PROTEINS')#BZR,COX2,REDDIT-BINARY,\n",
    "graph_label=load_label('PROTEINS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074e40bd",
   "metadata": {},
   "source": [
    "# Normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5cdc7aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k8/fv2pzvcn5p77dgds_c16lsl80000gn/T/ipykernel_69706/2588493156.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  (array - np.mean(array, axis=0)) / np.std(array, axis=0)\n"
     ]
    }
   ],
   "source": [
    "normalized_list = [\n",
    "    (array - np.mean(array, axis=0)) / np.std(array, axis=0)\n",
    "    for array in features]\n",
    "normalized_features=np.nan_to_num(normalized_list, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9a1a9f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113\n",
      "20\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = torch.tensor(normalized_features, dtype=torch.float32)\n",
    "y = torch.tensor(graph_label, dtype=torch.long)\n",
    "\n",
    "num_samples = len(X)\n",
    "print(num_samples)\n",
    "num_timesteps = len(X[0])\n",
    "print(num_timesteps)\n",
    "num_features = len(X[0][0])\n",
    "print(num_features)\n",
    "num_classes = len(np.unique(y))\n",
    "print(num_classes)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760b76d",
   "metadata": {},
   "source": [
    "# original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60d2fc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset: REDDIT-BINARY features\n",
      "2000\n",
      "19\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "features=load_dataset('REDDIT-BINARY')# BZR, PROTEINS,COX2,MUTAG,IMDB-BINARY,IMDB-MULTI\n",
    "graph_label=load_label('REDDIT-BINARY')\n",
    "\n",
    "X = torch.tensor(features, dtype=torch.float32)\n",
    "y = torch.tensor(graph_label, dtype=torch.long)\n",
    "\n",
    "num_samples = len(X)\n",
    "print(num_samples)\n",
    "num_timesteps = len(X[0])\n",
    "print(num_timesteps)\n",
    "num_features = len(X[0][0])\n",
    "print(num_features)\n",
    "num_classes = len(np.unique(y))\n",
    "print(num_classes)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "497bc527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e12bbf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████| 100/100 [01:59<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: Test Accuracy = 0.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████| 100/100 [02:00<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 2: Test Accuracy = 0.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████| 100/100 [02:00<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 3: Test Accuracy = 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████| 100/100 [02:00<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 4: Test Accuracy = 0.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████| 100/100 [02:01<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 5: Test Accuracy = 0.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████| 100/100 [02:01<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 6: Test Accuracy = 0.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████| 100/100 [02:02<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 7: Test Accuracy = 0.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████| 100/100 [02:02<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 8: Test Accuracy = 0.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████| 100/100 [02:01<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 9: Test Accuracy = 0.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████| 100/100 [02:02<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 10: Test Accuracy = 0.91%\n",
      "Final Accuracy using 10 fold CV: 89.75 ± 2.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_heads, n_layers, num_timesteps):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, num_timesteps, hidden_dim))\n",
    "        self.transformer = nn.Transformer(d_model=hidden_dim, nhead=n_heads, num_encoder_layers=n_layers, num_decoder_layers=n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim * num_timesteps, output_dim)  # Flatten the output of the transformer\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_emb = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        src_emb = src_emb.permute(1, 0, 2)  # (seq_len, batch, feature)\n",
    "        transformer_output = self.transformer.encoder(src_emb)\n",
    "        transformer_output = transformer_output.permute(1, 0, 2).contiguous().view(src.size(0), -1)  # Flatten\n",
    "        predictions = self.fc(transformer_output)\n",
    "        return predictions\n",
    "\n",
    "def reset_weights(model):\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "# Define input and output dimensions (example placeholders)\n",
    "input_dim = num_features\n",
    "hidden_dim = 16\n",
    "output_dim = num_classes\n",
    "n_heads = 2\n",
    "n_layers = 2\n",
    "num_timesteps = num_timesteps  # Adjust based on your sequence length\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = TransformerClassifier(input_dim, hidden_dim, output_dim, n_heads, n_layers, num_timesteps)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "loss_per_fold = []\n",
    "acc_per_fold = []\n",
    "pre_per_fold=[]\n",
    "rec_per_fold=[]\n",
    "f1_per_fold=[]\n",
    "fold_no = 1\n",
    "\n",
    "for train_idx, test_idx in kfold.split(X):\n",
    "    # Split data\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Train the model\n",
    "    reset_weights(model)\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(100), desc=\"Processing\"):\n",
    "        epoch_train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training loop\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track training loss and accuracy\n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_train += y_batch.size(0)\n",
    "            correct_train += (predicted == y_batch).sum().item()\n",
    "\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        model.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                output = model(X_batch)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total_test += y_batch.size(0)\n",
    "                correct_test += (predicted == y_batch).sum().item()\n",
    "\n",
    "                # Store predictions and targets for metrics\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        test_accuracy = correct_test / total_test\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        precision = precision_score(all_targets, all_preds, average='weighted',zero_division=0)\n",
    "        recall = recall_score(all_targets, all_preds, average='weighted',zero_division=0)\n",
    "        f1 = f1_score(all_targets, all_preds, average='weighted',zero_division=0)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        model.train()  # Switch back to training mode\n",
    "\n",
    "        # Print metrics for this epoch\n",
    "        #print(f'Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Train Accuracy = {train_accuracy:.2f}%, Test Accuracy = {test_accuracy:.2f}%')\n",
    "        #print(f'Precision = {precision:.2f}, Recall = {recall:.2f}, F1-Score = {f1:.2f}')\n",
    "    #print(f'Score for fold {fold_no}: ')\n",
    "    #accuracy=print_stat(train_accuracies,test_accuracies)\n",
    "    accuracy=np.max(test_accuracies)\n",
    "    pre=np.max(precisions)\n",
    "    rec=np.max(recalls)\n",
    "    f1=np.max(f1_scores)\n",
    "    acc_per_fold.append(accuracy)\n",
    "    pre_per_fold.append(pre)\n",
    "    rec_per_fold.append(rec)\n",
    "    f1_per_fold.append(f1)\n",
    "    \n",
    "    \n",
    "    print(f'Score for fold {fold_no}: Test Accuracy = {accuracy:.2f}%')\n",
    "#     with open(\"out_protiens.txt\", \"w\") as file:\n",
    "#         with redirect_stdout(file):\n",
    "#             print(f'Score for fold {fold_no}: Test Accuracy = {accuracy:.2f}%')\n",
    "    fold_no += 1\n",
    "stat(acc_per_fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "be8a966d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy using 10 fold CV: 90.31 ± 1.88%\n",
      "Final Accuracy using 10 fold CV: 89.75 ± 2.18%\n",
      "Final Accuracy using 10 fold CV: 89.72 ± 2.22%\n"
     ]
    }
   ],
   "source": [
    "stat(pre_per_fold)\n",
    "stat(rec_per_fold)\n",
    "stat(f1_per_fold)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73d536",
   "metadata": {},
   "source": [
    "# random spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2182e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
